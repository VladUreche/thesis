\section{Evaluation}
\label{sec-evaluation}

%%%%%%%%%%%% STUFF FOR TABLE %%%%%%%%%%%% 
\newcolumntype{C}{>{\centering\arraybackslash}p{14ex}}
%\newcolumntype{X}{>{\centering\arraybackslash}p{5ex}}
%\newcommand{\optpm}[1]{$\pm$ #1}
%\newcommand{\optpm}[1]{}
\newcommand{\sctx}[0]{Single Context}
\newcommand{\mctx}[0]{Multi Context}
\newcommand{\bn}[1]{\textbf{#1}}
\newcommand{\opt}[1]{#1}
%\newcommand{\opt}[1]{}

This section presents the results obtained by the miniboxing transformation. It will first present the miniboxing compiler plug-in and the miniboxing classloader (\S\ref{subsec-eval-impl}). Next, it will present the benchmarking infrastructure (\S\ref{subsec-eval-infrastructure}) and the benchmark targets (\S\ref{subsec-eval-targets}). Finally, it will present the results (\S\ref{subsec-eval-results} - \S\ref{subsec-eval-other-vms}) and draw conclusions (\S\ref{subsec-eval-remarks}).

\subsection{Implementation}
\label{subsec-eval-impl}

\topic{The miniboxing plug-in adds a code transformation phase in the Scala compiler.} Like specialization, the miniboxing phase is composed of two steps: transforming signatures and transforming trees. As the signatures are specialized, metadata is stored on exactly how the trees need to be transformed. This metadata later guides the tree transformation in duplicating and adapting the trees to obtain the miniboxed code. The duplication step reuses the infrastructure from specialization, with a second adaptation step which transforms storage from generic to miniboxed representation.   

The plugin performs several transformations:
\begin{packed_item}
\item Code duplication and adaptation, where values of type |T| are replaced by long integers and are un-miniboxed back to |T| at use sites (\S\ref{sec-mb-traf-type-bytes});
\item Rewiring methods like |toString|, |hashCode|, |equals| and array operations to use the runtime support (\S\ref{sec-mb-traf-runtime});
\item Opportunistic rewiring: new instance creation, specialized parent classes and method invocations (\S\ref{subsec-spec-rewiring});
\item Peephole minibox/un-minibox reduction (\S\ref{sec-mb-traf-peephole}).
\end{packed_item}
 
\topic{The miniboxing classloader} duplicates classes and performs the specialized class rewiring. It uses transformations from an experimental Scala backend to perform constant propagation and dead code elimination in order to remove switches on the type byte. It supports miniboxed classes generated by the current plug-in and in the current release only works for a single specialized type parameter. Also, the infrastructure for the double factory instantiation was written and tuned by hand, and may be integrated in the plug-in in a future release. We did not implement the |@loadtimeSpec| annotation yet.
 
\topic{The project also contains code for testing the plug-in and the classloader and performing microbenchmarks,} something which turned out to be more difficult than expected.  

\subsection{Benchmarking Infrastructure}
\label{subsec-eval-infrastructure}

\topic{The miniboxing plug-in produces bytecode which is then executed by the HotSpot Java Virtual Machine.} Although the virtual machine provides useful services to the running program, such as compilation, deoptimization and garbage collection, these operations influence our microbenchmarks by delaying or even changing the benchmarked code altogether. Furthermore, the non-deterministic nature of such events make proper benchmarking harder \cite{rigorous-java-benchmarking}.

\topic{In order to have reliable results for our microbenchmarks, we used ScalaMeter \cite{scalameter},} a tool specifically designed to reduce benchmarking noise. ScalaMeter is currently used in performance-testing the Scala standard library. When benchmarking, it forks a new virtual machine such that fresh code caches and type profiles are created. It then warms up the benchmarked code until the virtual machine compiles it down to native code using the C2 (server) \cite{hotspot-c2} compiler. When the code has been compiled and the benchmark reaches a steady state, ScalaMeter measures several execution runs. The process is repeated several times, 100 in our case, reducing the benchmark noise. For the report, we present the average of the measurements performed.

\topic{We ran the benchmarks} on an 8-core i7 machine running at 3.40GHz with 16GB of RAM memory. The machine ran a 64 bit version of Linux Ubuntu 12.04.2. For the Java Virtual Machine we used the Oracle Java SE Runtime Environment build 1.7.0\_11 using the C2 (server) compiler. The following section will describe the benchmarks we ran.

\begin{table*}[bp]
\centering
\small
\begin{tabular}{l|C|C|C|C|C|C}
                  & \multicolumn{2}{c|}{\texttt{ArrayBuffer.append}} & \multicolumn{2}{c|}{\texttt{ArrayBuffer.reverse}} & \multicolumn{2}{c}{\texttt{ArrayBuffer.contains}} \\\hline 
                  & \sctx            & \mctx            & \sctx            & \mctx            & \sctx                & \mctx            \\\hline
generic           & 50.1 \optpm{1.2} & 48.0 \optpm{0.9} & 20.4 \optpm{3.8} & 21.5 \optpm{2.3} & 1580.1 \optpm{ 12.0} &  3628.8 \optpm{ 40.2} \\
\opt{mb. switch        & 30.9 \optpm{2.0} & 35.5 \optpm{2.4} &  2.5 \optpm{0.5} & 15.1 \optpm{3.5} &  161.5 \optpm{  3.4} &   554.3 \optpm{  3.2} \\}
\opt{mb. dispatch      & 16.5 \optpm{1.3} & 58.2 \optpm{3.0} &  2.1 \optpm{0.6} & 26.5 \optpm{1.9} &  160.7 \optpm{  3.0} &  2551.6 \optpm{ 33.1} \\}
\rowcolor{Gray}
\bn{mb. switch + LS}   & \bn{15.6} \optpm{1.8} & \bn{14.8} \optpm{1.5} &  \bn{2.5} \optpm{0.5} &  \bn{2.4} \optpm{0.5} &  \bn{159.9} \optpm{  3.2} &   \bn{161.7} \optpm{  3.4} \\
\rowcolor{Gray}
\bn{mb. dispatch + LS} & \bn{15.1} \optpm{0.9} & \bn{15.9} \optpm{1.5} &  \bn{2.0} \optpm{0.6} &  \bn{2.7} \optpm{0.1} &  \bn{161.8} \optpm{  2.2} &   \bn{161.3} \optpm{  2.8} \\
specialization    & 39.7 \optpm{3.0} & 38.5 \optpm{3.0} &  2.0 \optpm{0.6} &  2.4 \optpm{0.5} &  155.8 \optpm{  2.6} &   156.3 \optpm{  3.4} \\
monomorphic       & 16.2 \optpm{1.0} & N/A           &  2.1 \optpm{0.6} & N/A           &  157.7 \optpm{  4.1} &                N/A \\
\end{tabular}


\begin{tabular}{l|C|C|C|C|C|C}
                  & \multicolumn{2}{c|}{\texttt{List} creation} & \multicolumn{2}{c|}{\texttt{List.hashCode}} & \multicolumn{2}{c}{\texttt{List.contains}} \\\hline 
                  & \sctx            & \mctx            & \sctx            & \mctx            & \sctx                & \mctx            \\\hline
generic           & 16.7 \optpm{   0.2} & 1841 \optpm{ 1068} & 22.1 \optpm{1.9} & 20.4 \optpm{0.8} & 1739.5 \optpm{ 4.9} & 2472.4 \optpm{49.0} \\
\opt{mb. switch        & 11.4 \optpm{   0.1} &  11.7 \optpm{   0.6} & 18.3 \optpm{2.2} & 18.8 \optpm{2.2} & 1438.2 \optpm{17.5} & 1443.2 \optpm{21.8} \\}
\opt{mb. dispatch      & 11.4 \optpm{   0.1} &  11.5 \optpm{   0.4} & 15.6 \optpm{1.8} & 21.0 \optpm{1.9} & 1369.1 \optpm{ 8.0} & 1753.2 \optpm{16.9} \\}
\rowcolor{Gray}
\bn{mb. switch + LS}   & \bn{11.5} \optpm{   0.6} &  \bn{11.6} \optpm{   0.5} & \bn{16.2} \optpm{2.4} & \bn{16.1} \optpm{2.2} & \bn{1434.9} \optpm{20.4} & \bn{1446.3} \optpm{19.6} \\
\rowcolor{Gray}
\bn{mb. dispatch + LS} & \bn{12.1} \optpm{   0.5} &  \bn{12.7} \optpm{   1.9} & \bn{16.1} \optpm{2.1} & \bn{15.3} \optpm{1.7} & \bn{1364.2} \optpm{ 7.8} & \bn{1325.9} \optpm{44.4} \\
specialization    & 11.4 \optpm{   0.1} &  11.4 \optpm{   0.2} & 14.5 \optpm{1.4} & 36.4 \optpm{2.5} & 1341.0 \optpm{37.8} & 1359.2 \optpm{23.4} \\
monomorphic       & 10.2 \optpm{   1.2} &                 N/A  & 13.3 \optpm{0.8} & N/A              & 1172.0 \optpm{ 3.4} & N/A                 \\
\end{tabular}
\caption{Benchmark running times. The benchmarking setup is presented in \S\ref{subsec-eval-infrastructure} and the targets are presented in \S\ref{subsec-eval-targets}. The time is measured in milliseconds.}
\label{tbl-results-main}
\end{table*}


\subsection{Benchmark Targets}
\label{subsec-eval-targets}

We executed the benchmarks in two scenarios:
\begin{packed_item}
\item ``Single Context'' corresponds to the benchmark target (|ArrayBuffer| or |ListNode|) executed with a single value type, |Int|;
\item ``Multi Context'' corresponds to running the benchmark for all value types and only then measuring the execution time for the target value type, |Int|;
\end{packed_item}  
\vspace{0mm}
The benchmarks were executed with 7 transformations:
\begin{packed_item}
\item {generic}: the generic version of the code, uses boxing;
\opt{\item {mb. switch}: miniboxed, using the type byte switching};
\opt{\item {mb. dispatcher}: miniboxed, dispatcher runtime support};
\item {mb. switch + LS}: miniboxed, type byte switching, load-time specialization with the double factory mechanism; 
\item {mb. dispatcher + LS}: miniboxed, dispatcher, load-time specialization with the double factory mechanism;
\item {specialized}: code transformed by specialization;
\item {monomorphic}: code specialized by hand, which does not need the redirects generated by specialization.
\end{packed_item}
\vspace{-1mm}

\topic{For the benchmarks, we used the two classes presented in the previous sections:} The |ArrayBuffer| class simulates collections and algorithms which make heavy use of bulk storage and the |ListNode| class simulates collections which require random heap access. We chose the benchmark methods such that each tested a certain feature of the miniboxing transformation. We used very small methods such that any slowdowns can easily be attributed to bytecode or can be diagnosed in a debug build of the virtual machine, using the compilation and deoptimization outputs. 

|ArrayBuffer.append| creates a new array buffer and appends 3 million elements to it. This benchmark tests the array writing operations in isolation, such that they cannot be grouped together and optimized. 

|ArrayBuffer.reverse| reverses a 3 million element array buffer. This benchmark proved the most difficult in terms of matching the monomorphic code performance.

|ArrayBuffer.contains| checks for the existence of elements inside an initialized array buffer. It exercises the |equals| method rewiring and revealed to us that the initial transformation for |equals| was suboptimal, as we were not using the information that two miniboxed values were of the same type. This benchmark showed a 22x speedup over generic code. 

List construction builds a 3 million element linked list using |ListNode| instances. This benchmark verifies the speed of miniboxed class instantiation. It was heavily slowed down by the reflective instantiation, therefore we introduced the double factory for class instantiation using the classloader.

|List.hashCode| computes the hash code of a list of 3 million elements. We used this benchmark to check the performance of the |hashCode| rewiring. It was a surprise to see the |hashCode| performance for generic code running in the interpreter (Table \ref{tbl-results-interp}). It is almost one order of magnitude faster than specialized code and 5 times faster than miniboxing. The explanation is that computing the hash code requires boxing and calling the |hashCode| method on the boxed object. When the benchmarks are compiled and optimized, this is avoided by inlining and escape analysis, but in the interpreter, the actual object allocation and call to |hashCode| do happen, making the heterogeneous translation slower.
 
|List.contains| tests whether a list contains an element, repeated for 3 million elements. It tests random heap access and the performance of the |equals| operator rewiring.

\subsection{Benchmark Results}
\label{subsec-eval-results}

Table \ref{tbl-results-main} presents the main results of our benchmarks. The table highlights  ``mb. switch + LS'' and ``mb. dispatch + LS'', which represent the miniboxing encoding using the load-time specialization invoked with the double factory mechanism.

The miniboxing encoding based on type tag switching, ``mb. switch + LS'', offers steady performance close to that of specialization and monomorphic code, with slowdowns ranging between 0 and 20 percent. The classloader specialization, coupled with constant propagation and dead code elimination, make the type tag switching approach the most stable across multiple executions with different type arguments, with at most 6 percent difference between ``Single Context'' and ``Multi Context'', in the case of |ArrayBuffer.append|.    

The dispatcher-based encoding, ``mb. dispatch + LS'', also offers performance close to specialization and mono\-morphic code, with slightly better performance when traversing the linked list (benchmarks |hashCode| and |contains|), and a lower performance on |List| creation. This suggests that passing the dispatcher object on the stack is more expensive than passing a type tag. 

It is worth noting that the dispatcher-based implementation relies on inlining performed by the just-in-time compiler. Although the load-time cloning mechanism ensures type profiles remain monomorphic, the burden of inlining falls on the just-in-time compiler. In the case of virtual machines that perform ahead-of-time compilation, such as Excelsior JET \cite{excelsior-jet}, the newly specialized class is compiled to native code without interpretation, thus no type profiles are available and no inlining takes place for the miniboxing runtime. In contrast to dispatching, type tag switching only requires loading-time constant propagation and dead code elimination to remove the overhead of the miniboxing runtime. This makes it a better candidate for robust performance across different virtual machines. The next section will present interpreter benchmarks.

\subsection{Interpreter Benchmarks}
\label{subsec-eval-interpreter}

Before compiling the bytecode to native machine code, the HotSpot Virtual Machine interprets it and gathers profiles that later guide compilation. Table \ref{tbl-results-interp} presents results for running the same set of benchmarks in the interpreter, without compilation. It is important that transformations do not visibly degrade performance in the interpreter, as this slows down application startup. The data highlights a steady behavior for the the type tag switching, while the dispatcher-based approach suffers from up to 4x slowdowns.

\begin{table}[t!]
\centering
\small
\begin{tabular}{l|c|c|c|c|c|c}
                  & \multicolumn{3}{c|}{\texttt{ArrayBuffer}} & \multicolumn{3}{c}{\texttt{List}} \\\hline 
generic           & 4.6 & 2.2 & 367.0 & 1.4 & \textbf{0.2} & 16.6 \\
\rowcolor{Gray}
\bn{mb. switch + LS}   & \bn{1.6} & \bn{0.3} &  \bn{25.0} & \bn{0.8} & \bn{1.3} &  \bn{4.2} \\
mb. dispatch + LS & 2.5 & 0.7 &  88.9 & 1.1 & 1.5 &  7.3 \\
specialization    & 4.3 & 0.5 &  30.7 & 0.6 & 1.9 &  2.2 \\
monomorphic       & 1.0 & 0.2 &  12.7 & 0.4 & 1.2 &  2.2 \\
\end{tabular}
\caption{Running time for the benchmarks in the HotSpot Java Virtual Machine interpreter. The time is measured in seconds as instead of milliseconds as in the other tables. ``Single context'' and ``Multi context'' have similar results.}
\label{tbl-results-interp}
\end{table}

The data shows a consistent slowdown of the tag switching approach compared to the monomorphic code in 4 of the 6 experiments. This can most likely be attributed to the mechanism for invoking object methods, which requires loading a reference to the module from a static field and then performing a method call. Even after the method call is inlined, the Scala backend (and the load-time specializer) do not remove the static field access, thus leaving the redundant but possibly side-effecting instruction in the hot loop. In the native code the field access is compiled away by the just-in-time compiler. This could be improved in the Scala backend. 

\begin{table}[t!]
\centering
\small
\begin{tabular}{l|r|r|g|r}
                      &   erasure  &   dispatch &    switch &        spec. \\\hline 
ArrayBuffer           &        4.4 &       19.5 &      24.5 &         57.6 \\
ArrayBuffer factory   &         -- &      + 9.0 &     + 8.5 &           -- \\
ListNode              &        3.1 &       10.9 &      11.5 &         45.0 \\
ListNode factory      &         -- &      + 8.7 &     + 8.3 &           -- \\
\end{tabular}
\caption{Bytecode generated by different translations, in kilobytes. Factories add extra bytecode for the double factory mechanism. ``spec.'' stands for specialization.}
\label{tbl-results-bytecode}
\end{table}

\begin{table}[b!]
\centering
\small
\begin{tabular}{l|r|r}
                               &  bytecode size (KB) & classes \\\hline 
Spire - specialized (current)  &               13476 &    2545 \\
\rowcolor{Gray}
Spire - miniboxed              &                4820 &    1807 \\
Spire - generic                &                3936 &    1530 \\
\end{tabular}
\caption{Bytecode generated by using specialization, miniboxing and leaving generic code in the Spire numeric abstractions library.}
\label{tbl-results-bytecode-spire}
\end{table}

\subsection{Bytecode Size}
\label{subsec-eval-size}

Table \ref{tbl-results-bytecode} presents the bytecode generated for |ArrayBuffer| and |ListNode| by 4 transformations: erasure, miniboxing with dispatcher, miniboxing with switching and specialization. The fraction of bytecode created by miniboxing, when compared to specialization, lies between 0.2x to 0.4x. This is marginally better than the fraction we expected, 0.4x, which corresponds to $4^n / 10^n$ for $n=1$. The reason the fraction is $4^n / 10^n$ instead of $2^n / 10^n$ is explained in \S \ref{sec-mb-traf-inheritance}. The double factory mechanism adds a significant bytecode, in the order of 10 kilobytes per class.

In order to evaluate the benefits of using the miniboxing encoding for real-world software, we developed a ``specialization-hijacking'' mode, where specialization was turned off and all |@specialized| notations were treated as |@minispec|, thus triggering miniboxing on all methods and classes where specialization was used. For this benchmark we only used the switching-based transformation.
 
The first evaluation was performed on Spire \cite{erik-spire}, a Scala library providing abstractions for numeric types, ranging from boolean algebras to complex number algorithms. Spire is the one library in the Scala community which uses specialization the most, and the project owner, Erik Osheim, contributed numerous bug fixes and enhancements to the Scala compiler in the area of specialization. The results, presented in Table \ref{tbl-results-bytecode-spire}, show a bytecode reduction of 2.8x and a 1.4x, or 40\%, reduction in the number of specialized classes. The two reductions are not proportional because specialized methods inflate the code size of classes, but do not increase the class count. The bytecode reduction is limited to 2.8x because specialization is used in a directed manner, pointing exactly to the value types which should be specialized. So, instead of generating 10 classes per type parameter, it only generates the necessary value types. Nevertheless, even starting from manually directed specialization, the miniboxing transformation is able to further reduce the bytecode size.    

The second evaluation, shown in Table \ref{tbl-results-bytecode-vector}, is motivated by a common complaint in the Scala community: that the collections in the standard library should be specialized. To perform an evaluation on collections, we sliced a part of the library around the |Vector| class and examined the impact of using the specialization and miniboxing transformations. On the approximately 64 Scala classes, traits and objects included in our slice, the bytecode reduction obtained by miniboxing compared to specialization is 4.7x. Compared to the generic |Vector|, the miniboxing code growth is 1.7x, opposed to almost 8x for specialization. 

\begin{table}[b!]
\centering
\small
\begin{tabular}{l|r|r}
                               &  bytecode size (KB) & classes \\\hline 
Vector - specialized           &                5691 &    1434 \\
\rowcolor{Gray}
Vector - miniboxed             &                1210 &     435 \\
Vector - generic (current)     &                 715 &     223 \\
\end{tabular}
\caption{Bytecode generated by using specialization, miniboxing and leaving generic code on the Scala collection library slice around Vector.}
\label{tbl-results-bytecode-vector}
\end{table}

\subsection{Load-time Specialization Overhead}
\label{subsec-eval-specialization}

In this section we will evaluate the overhead of the double factory mechanism. There are three types of overhead involved:
\begin{packed_item}
  \item Bytecode overhead, shown in the previous section;
  \item Time spent specializing and loading a class;
  \item Heap overhead for the classloader and factory.
\end{packed_item}

We will further explore the last two sources of overhead.

\begin{table*}[bp]
\centering
\small
\begin{tabular}{l|C|C|C|C|C|C}
                       & \multicolumn{2}{c|}{\texttt{ArrayBuffer.append}} & \multicolumn{2}{c|}{\texttt{ArrayBuffer.reverse}} & \multicolumn{2}{c}{\texttt{ArrayBuffer.contains}} \\\hline 
                       & \sctx            & \mctx            & \sctx            & \mctx            & \sctx                & \mctx            \\\hline
generic                & 78.3 \optpm{12.6}& 52.3 \optpm{2.2} &  3.2 \optpm{1.0} & 20.3 \optpm{1.8} &  607.6 \optpm{  0.6} &  3146.1 \optpm{  4.8} \\
\opt{mb. switch        & 27.6 \optpm{13.8}&         $\times$ &  7.4 \optpm{1.5} &         $\times$ &  844.4 \optpm{  1.0} &              $\times$ \\}
\opt{mb. dispatch      & 27.0 \optpm{2.0} & 34.8 \optpm{1.6} &  3.2 \optpm{0.1} & 10.8 \optpm{1.5} &  844.7 \optpm{  0.6} &   962.7 \optpm{ 1.3} \\}
\rowcolor{Gray}
\bn{mb. switch + LS}   & \bn{22.2} \optpm{9.3} & \bn{14.3} \optpm{1.3} &  \bn{3.8} \optpm{0.8} &  \bn{2.9} \optpm{0.1} &  \bn{725.4} \optpm{  0.7} &   \bn{725.2} \optpm{  0.5} \\
\rowcolor{Gray}
\bn{mb. dispatch + LS} & \bn{32.9} \optpm{8.2} & \bn{26.4} \optpm{2.1} &  \bn{3.4} \optpm{0.3} &  \bn{4.0} \optpm{0.9} &  \bn{844.6} \optpm{  0.9} &   \bn{845.3} \optpm{  3.0} \\
specialization         & 21.7 \optpm{8.3} & 13.4 \optpm{2.1} &  3.5 \optpm{0.7} &  2.7 \optpm{0.8} &  488.7 \optpm{  0.9} &   489.4 \optpm{  2.5} \\
monomorphic            & 19.8 \optpm{5.9} & N/A           &  3.1 \optpm{0.5} & N/A           &  490.4 \optpm{  3.0} &                N/A \\
\end{tabular}
                 
\begin{tabular}{l|C|C|C|C|C|C}
                       & \multicolumn{2}{c|}{\texttt{List} creation} & \multicolumn{2}{c|}{\texttt{List.hashCode}} & \multicolumn{2}{c}{\texttt{List.contains}} \\\hline 
                       & \sctx            & \mctx            & \sctx            & \mctx            & \sctx                & \mctx            \\\hline
generic                & 32.6 \optpm{   7.5} &  23.3 \optpm{ 1.5} & 13.4 \optpm{1.7} & 13.6 \optpm{1.2} & 1846.5 \optpm{ 4.9} & 2168.1 \optpm{4.0} \\
\opt{mb. switch        & 23.7 \optpm{   1.2} &  18.0 \optpm{ 0.4} & 11.7 \optpm{1.0} & 10.9 \optpm{0.4} & 1420.8 \optpm{15.3} & 1421.5 \optpm{16.0} \\}
\opt{mb. dispatch      & 20.9 \optpm{   1.9} &  18.3 \optpm{ 1.0} & 12.4 \optpm{0.9} & 11.4 \optpm{0.4} & 1359.3 \optpm{24.9} & 1427.5 \optpm{16.8} \\}
\rowcolor{Gray}
\bn{mb. switch + LS}   & \bn{23.2} \optpm{   0.7} &  \bn{17.1} \optpm{   0.8} & \bn{12.2} \optpm{0.6} & \bn{10.5} \optpm{0.5} & \bn{1414.8} \optpm{4.0} & \bn{1459.4} \optpm{22.0} \\
\rowcolor{Gray}
\bn{mb. dispatch + LS} & \bn{25.0} \optpm{   0.5} &  \bn{18.3} \optpm{   1.2} & \bn{12.1} \optpm{0.6} & \bn{10.5} \optpm{0.3} & \bn{1390.6} \optpm{ 7.1} & \bn{1402.9} \optpm{12.6} \\
specializare          & 21.7 \optpm{   0.7} &  16.9 \optpm{ 0.6} & 12.4 \optpm{0.7} & 10.6 \optpm{0.2} & 1463.5 \optpm{14.4} & 1459.8 \optpm{8.9} \\
monomorphic             & 19.6 \optpm{   3.9} &               N/A  & 11.7 \optpm{3.9} & N/A              & 1249.2 \optpm{ 1.9} & N/A                 \\
\end{tabular}

\caption{Running times on the Graal Virtual Machine. ``$\times$'' marks benchmarks for which the bytecode generated crashed the Graal just-in-time compiler. The time is measured in milliseconds.}
\label{tbl-results-graal}
\end{table*}

\begin{table}[t!]
\centering
\small
\begin{tabular}{l @{\hspace{11mm}} |r|r}
                                    & time in ms   & classes \\\hline 
classpath - just load               &  182 $\pm$ 5 &     9 $\times$ 25 = 225 \\
classloader - warmed up             &  300 $\pm$ 4 &     225 \\
classloader - cold start            &  461 $\pm$ 9 &     225 \\
\end{tabular}
\caption{Loading time (classpath) and time for cloning and specialization (classloader) for the 9 specialized variants of Vector and their transitive dependencies.}
\label{tbl-results-classloading-load}
\end{table}

\begin{table}[t!]
\centering
\small
\begin{tabular}{l @{\hspace{7mm}} |r|r}
                               & time in ms   & classes \\\hline 
classpath - new                &  258 $\pm$ 5 &     9 $\times$ 42 = 378 \\
classpath - factory            &  268 $\pm$ 6 &     378 \\
classloader - factory - warm   &  488 $\pm$10 &     378 \\
classloader - factory - cold   &  655 $\pm$ 9 &     378 \\
\end{tabular}
\caption{Instantiation time for the 9 specialized variants of Vector and their transitive dependencies.}
\label{tbl-results-classloading-inst}
\end{table}

\subsubsection{Time Spent Specializing}

Table \ref{tbl-results-main}, in the ``|List| creation'' column, shows the overhead of the double factory mechanism and class specialization is not statistically noticeable after the mechanism is warmed up. Nevertheless, it is important to understand how the mechanism behaves during a cold start, as this directly impacts an application's startup time. In this subsection we will examine the overhead for a cold start, coming from two different sources:

\begin{packed_item}
  \item The runtime class specialization;
  \item The cold start of the double factory mechanism.
\end{packed_item}

The evaluation checks the two overheads separately: in the first experiment we only load the classes (using |Class.forName|) to trigger the runtime class specialization, while in the second experiment we instantiate the classes, either directly, using the |new| operator or through the double factory mechanism. In order to evaluate the class specialization, we instrumented the specializing classloader to dump the resulting class files, such that we can compare the specializing classloader to simply loading the specialized variants from the classpath. 

For the comparison, we use the |Vector| class described in the previous section. The |Vector| class mixes in 36 traits \cite{scalable-component-abstractions} which are translated by the Scala compiler as transitive dependencies of the class. In our experiments, loading the |Vector| class using |Class.forName| transitively loaded another 24 specialized classes for each variant. Instantiating a vector using |new| further loads another 18 classes, mainly specialized trait implementations and internal classes, leading to a total of 42 classes loaded with each specialized variant of |Vector|. 

In each experiment we start the virtual machine, start counting the time, load or instantiate |Vector| for all 9 value types in Scala, output the elapsed time and exit. Once a class is loaded, its internal representation in the virtual machine remains cached until its classloader is garbage collected. In order to perform correct benchmarks, we chose to use a virtual machine to load the 9 specialized variants of |Vector| only once, and then restart the virtual machine. We repeated the process 100 times for each measurement. 

The first experiment involves loading the class: this can be done either by using the specializing classloader to instantiate a template or by loading the class file dumped from a previous specialization run. We observed a significant difference between cold starting the specializing classloader and warming it up on a different set of classes. This is shown in Table \ref{tbl-results-classloading-load}: cold starting the specialization classloader incurs a slowdown of 153\% while warming it up before leads to a 65\% slowdown in class loading time.   

The second experiment involves instantiating the class, either directly (using the |new| operator) or through the double factory mechanism. Table \ref{tbl-results-classloading-inst} presents the results. The surprising result of this experiment is that the overhead caused by the double factory mechanism is under 4\%. As before, most of the time is spent specializing the template to produce the specialized class, which, depending on whether the classloader was used before, can lead to a slowdown between 84\% and 144\%. It is important to point out this overhead is a one-time cost, and further instantiations of the specialized variants take on the order of tens of milliseconds. 

\subsubsection{Heap Overhead}

In this section we will attempt to bound the heap usage of the double factory mechanism. The double factory mechanism consists of a first level factory, which uses reflection to create second level factories, which, in turn, use the new operator to instantiate load-time specialized classes. This mechanism was imposed in order to avoid the cost of reflection-based instantiation, which we found to be more expensive in terms of overhead. Each second level factory corresponds to a set of pre-determined type tags, thus instantiating two specialized variants will require two separate second level factories.

The first level factory mechanism keeps a cache of $10^n$ references pointing to second level factories, which is initially empty and fills up as the different variants are created. The second level factories are completely stateless and only offer a method for each specialized class constructor. Therefore the maximum heap consumption, for a 64 bit system running the HotSpot Virtual Machine, would be 16 bytes for each second level factory and 8 bytes for its cached reference, all times $10^n$, assuming all variants are loaded. This means a total of $24 \times 10^n$ bytes of storage. For a class with a single type parameter, this would mean a heap overhead in the order of hundreds of bytes. Assuming all of spire's specialized classes used arrays and required the two factory mechanism, since most take a single type parameter, it would mean a heap overhead in the order of tens of kilobytes.

However a hidden overhead is also present, consisting of the internal class representations for the second level factories inside the virtual machine. To bound this overhead, we can compare the factories to the classes themselves: for each specialized variant of the class there will be a specialized factory, with a method corresponding to each constructor of the class. The factory will therefore always have a strictly smaller internal representation than the specialized class, leading to at most a doubling of the internal class representation in the virtual machine.

\subsection{Extending to Other Virtual Machines}
\label{subsec-eval-other-vms}

In order to asses whether the miniboxing runtime system provides good performance on other virtual machines, we have evaluated it on Graal \cite{graal}. The Graal Virtual Machine consists of the same interpreter as the HotSpot Virtual Machine but a completely rewritten just-in-time compiler. Since the interpreter is the same, the same type profiles and hotness information is recorded, but the code is compiled using different transformations and heuristics. The results in Table \ref{tbl-results-graal} exhibit both a much lower variability but also a lower peak performance compared to the C2 compiler in HotSpot (in Table \ref{tbl-results-main}). With the single exception of |ArrayBuffer|'s |contains| benchmark, the switching runtime support with class loading behaves similarly to specialized code.        

\subsection{Evaluation Remarks}
\label{subsec-eval-remarks}

After analyzing the benchmarking results, we believe the miniboxing transformation with type byte switching and classloader duplication provides the most stable results and fulfills our initial goal of providing an alternative encoding for specialization, which produces less bytecode without sacrificing performance. Using the classloader for duplication and switch elimination, the type byte switching does not require forced inlining, making the transformation work without any inlining support from the Scala compiler.
