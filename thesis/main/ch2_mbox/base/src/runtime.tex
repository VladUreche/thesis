\section{Miniboxing Bulk Storage Optimization}
\label{mbox:sec-runtime}

Arrays are Java's bulk storage facility. They can store value types or references to heap objects. This is done efficiently, as values are stored one after the other in contiguous blocks of memory and access is done in constant time. Their characteristics make arrays good candidates for internal data structures in collections and algorithms.  

But in order to implement compact storage and constant access overhead, arrays are monomorphic under the hood, with separate (and incompatible) variants for each of the primitive value types. What's more, each array type has its own specific bytecode instructions to manipulate it. 

The goal we set forth was to match the performance of monomorphic arrays in the context of miniboxing-encoded values. To this end, we had two alternatives to implementing arrays for miniboxed values: use arrays of long integers to store the encoded values or use monomorphic arrays for each type, and encode or decode values at each access.

\topic{Storing encoded values in arrays provides} the advantage of uniformity: all the code in the minibox-specialized class uses the |Long| representation and array access is done in a single instruction. Although this representation wastes heap space, especially for small value types such as boolean or byte, this is not the main drawback: it is incompatible with the rest of the Scala code. 

In order to stay compatible with Java, Scala code uses monomorphic arrays for each value type. Therefore arrays of long integers in miniboxed classes must not be allowed to escape from the transformed class, otherwise they may crash outside code attempting to read or write them. To maintain compatibility, we could convert escaping arrays to their monomorphic forms. But the conversion would introduce delays and would break aliasing, as writes from the outside code would not be visible in the miniboxed code and vice versa. Since completely prohibiting escaping arrays severely restricts the programs that can use miniboxing, this solution becomes unusable in practice.

\topic{Thus, the only choice left is to use arrays in their mo\-no\-mor\-phic format for each value type,} so we maintain compatibility with the rest of the Scala code. This decision led to another problem: any array access requires a call to the miniboxing runtime support which performs a dispatch on the type byte. Depending on the type byte's value, the array is cast to its correct type and the corresponding bytecode instruction for accessing it is used. This is followed by the encoding operation, which converts the read value to a long integer. The following snippet shows the array read operation implemented in the miniboxing runtime support code:

\begin{lstlisting-nobreak}
 def array_get[T](array: Array[T], idx: Int, tag: Byte): Minibox = tag match {
   case INT =>     
     array.asInstanceOf[Array[Int]](idx).toLong
   case LONG =>    
     array.asInstanceOf[Array[Long]](idx)
   case DOUBLE =>  Double.doubleToRawLongBits(
     array.asInstanceOf[Array[Double]](idx)).toLong
   ...
 }
\end{lstlisting-nobreak}

\topic{The most complicated and time-consuming part of our work involved rewriting the miniboxing runtime support to match the performance of specialized code.} The next subsections present the HotSpot Java Virtual Machine execution (\S\ref{mbox:subsec-mb-jvm}), the main benchmark we used for testing (\S\ref{mbox:subsec-mb-bench}) and two implementations for the runtime support: type byte switching (\S\ref{mbox:subsec-mb-switching}) and object-oriented dispatching (\S\ref{mbox:subsec-mb-dispatching}).

\subsection{HotSpot Execution}
\label{mbox:subsec-mb-jvm}

\topic{We used benchmarks to guide our implementation of the miniboxing runtime support.} In this section we will briefly present the just-in-time compilation and optimization me\-cha\-ni\-sms in the HotSpot Java Virtual Machine \cite{hotspot-c1, hotspot-c2}, since they directly influenced our design decisions. Although the work is based on the HotSpot Java Virtual Machine, we highlight the underlying mechanisms that interfere with miniboxing, in hope that our work can be used as the starting point for the a\-na\-ly\-sis on different virtual machines. % , with different heuristics and optimization mechanisms.

\topic{The HotSpot Java Virtual Machine} starts off by interpreting bytecode. After several executions, a method is considered ``hot'' and the just-in-time compiler is called in to transform it into native code. During compilation, aggressive inlining is done recursively on all the methods that have been both executed enough times and are small enough. Typical inlining requirements for the C2\footnote{We did not use tiered compilation.} (server) just-in-time compiler are 10000 executions and size below 35 bytes.

When inlining static calls, the code is inlined directly. For virtual and interface calls, however, the code depends on the receiver. To learn which code to inline, the virtual machine will profile receiver types during the interpretation phase. Then, if a single receiver is seen at runtime, the compiler will inline the method body from that receiver. This inlining may later become incorrect, if a different class is used as the receiver. For such a case the compiler inserts a guard: if the runtime type is not the one expected, it jumps back to interpretation mode. The bytecode may be compiled again later if it runs enough times, with both possible method bodies inlined. But if a third runtime receiver is seen, the call site is marked as megamorphic and inlining is not performed anymore, not even for the previous two method bodies.

After inlining as much code as feasible, the virtual machine's just-in-time compiler applies optimizations, which significantly reduce the running time, especially for array operations which are very regular and for which bounds checks can be eliminated.
\subsection{Benchmark}
\label{mbox:subsec-mb-bench}

We benchmarked the performance on the two examples previously shown before, |ListNode| and |ArrayBuffer|. Throughout benchmarking, one particular method stood out as the most sensitive to the runtime support implementation: the |reverse| method of the |ArrayBuffer| class. The rest of this section uses the |reverse| method to explore the performance of different implementations of the runtime support:   

\begin{lstlisting-nobreak}
 def reverse_M(T_Type_local: Byte): Unit = {
   var idx = 0
   val xdi = elemCount - 1
   while (idx < xdi) {
     val el1: Long = getElement_M(T_Type, idx)
     val el2: Long = getElement_M(T_Type, xdi)
     setElement_M(T_Type, idx, el2)
     setElement_M(T_Type, xdi, el1)
     idx += 1
     xdi -= 1
   }
 }
\end{lstlisting-nobreak}

%\newcommand{\optpm}[1]{\optpm{#1}
\newcommand{\optpm}[1]{}

\begin{table}[t]
\small
\begin{tabular}{l|r|r}
                                        &   Single Context          & Multi Context          \\\hline
generic                                 &            20.4 \optpm{3.7}&             21.5 \optpm{2.2}\\
miniboxed, no inlining                  &            34.5 \optpm{3.1}&             34.4 \optpm{2.2}\\
\rowcolor{Gray}
miniboxed, full switch                  &             2.4 \optpm{0.6}&             15.1 \optpm{3.5}\\
miniboxed, semi-switch                  &             2.4 \optpm{0.4}&             17.2 \optpm{3.0}\\
miniboxed, decision tree                &            24.2 \optpm{3.0}&             24.1 \optpm{2.9}\\
miniboxed, linear                       &            24.3 \optpm{3.0}&             22.9 \optpm{4.0}\\
\rowcolor{Gray}
miniboxed, dispatcher                   &             2.1 \optpm{0.6}&             26.4 \optpm{1.9}\\
specialized                             &             2.0 \optpm{0.6}&              2.4 \optpm{0.4}\\
monomorphic                             &             2.1 \optpm{0.6}&              N/A  \\
\end{tabular}
\caption{The time in milliseconds necessary for reversing an array buffer of 3 million integers. Performance varies based on how many value types have been used before (Single Context vs. Multi Context).}
\label{mbox:tbl-mb-simple}
\end{table}

The running times presented in table \ref{mbox:tbl-mb-simple} correspond to reversing an integer array buffer of 3 million elements. To put things into perspective, along with different designs, the table also provides running times for monomorphic code (specialized by hand), specialization-annotated code and generic code. Measurements are taken in two scenarios: For ``Single Context'', an array buffer of integers is created and populated and its |reverse| method is benchmarked. In ``Multi Context'', the array buffer is instantiated, populated and reversed for all primitive value types first. Then, a new array buffer of integers is created, populated and its |reverse| method is benchmarked. The HotSpot Java Virtual Machine optimizations are influenced by the historical paths executed in the program, so using other type arguments can have a drastic impact on performance, as can be seen from the table, where the times for ``Single Context'' and ``Multi Context'' are very different: this means the virtual machine gives up some of its optimizations after seeing multiple instantiations with different type arguments.  ``Multi Context'' is the likely scenario a library class will be in, as multiple instantiations with different type arguments may be created during execution.

\subsection{Type Byte Switching}
\label{mbox:subsec-mb-switching}

\topic{The first approach we tried, the simple switch on the type byte, quickly revealed a problem: The array runtime support methods were too large for the just in time compiler to inline at runtime}. This corresponds to the ``miniboxing, no inlining'' in table \ref{mbox:tbl-mb-simple}. To solve this problem, we tasked the Scala compiler with inlining runtime support methods in its backend, independently of the virtual machine. But this was not enough: the |reverse_M| method calls |getElement_M| and |setElement_M|, which also became large after inlining the runtime support, and were not inlined by the virtual machine. This required us to recursively mark methods for inlining between the runtime support and the final benchmarked method.

\topic{The forced inlining in the Scala backend produced good results.} The measurement, corresponding to the ``miniboxed, full switch'' row in the table, shows miniboxed code working at almost the same speed as specialized and monomorphic code. This can be explained by the  loop unswitching optimization in the just-in-time compiler. With all the code inlined by the Scala backend, loop unswitching was able to hoist the type byte switch statement outside the while loop. It then duplicated the loop contents for each case in the switch, allowing array-specific optimizations to bring the running time close to monomorphic code.

\topic{But using more primitive types as type arguments diminished the benefit.} We tested the |reverse| operation in two situations, to check if the optimizations still take place after we use it on array buffers with different type arguments. It is frequently the case that the HotSpot Java Virtual Machine will compile a method with aggressive assumptions about which paths the execution may take. For the branches that are not taken, guards are left in place. Then, if a guard is violated during execution, the native code is interrupted and the program continues in the interpreter. The method may be compiled again later, if it is executed enough times to warrant compilation to native code. Still, upon recompilation, the path that was initially compiled to a stub now becomes a legitimate path and may preclude some optimizations. We traced this problem to the floating point encoding, specifically the bit-exact conversion from floating point numbers to integers, that, once executed, prevents loop unswitching.     

\topic{We tried different constructions for the miniboxing runtime support:} splitting the match into two parts and having an if expression that would select one or the other (``semi-switch''), transforming the switch into a decision tree (``decision tree'') and using a linear set of 9 if statements (``linear''), all of which appear in table \ref{mbox:tbl-mb-simple}. These new designs either degraded in the multiple context scenario, or provided a bad baseline performance from the beginning. What's more, the fact that the runtime ``remembered'' the type arguments a class was historically instantiated with made the translation unusable in practice, since this history is not only influenced by code explicitly called before the benchmark, but transitively by all code executed since the virtual machine started. % A different solution was necessary. 

\subsection{Dispatching}
\label{mbox:subsec-mb-dispatching}

\topic{The results obtained with type byte switching showed that we were committing to a type too late in the execution:} Forced inlining had to carry our large methods that covered all types inside the benchmarked method, where the optimizer had to hoist the switch outside the loop:

\begin{lstlisting-nobreak}
 while (...) {
   val el1: Long = T_Type match { ... }
   val el2: Long = T_Type match { ... }
   T_Type match { ... }
   T_Type match { ... }
 }
\end{lstlisting-nobreak}

\topic{Ideally, this switch should be done as early as possible, even as soon as class instantiation.} This can be done using an object-oriented approach: instead of passing a byte value during class instantiation and later switching on it, we can pass objects which encode the runtime operations for a single type, much like the where objects in PolyJ \cite{myers-polyj}. We call this object the dispatcher. The dispatcher for each value type encodes a common set of operations such as array get and set. For example, |IntDispatcher| encodes the operations for integers:

\begin{lstlisting-nobreak}
 abstract class Dispatcher {
   def array_get[T](arr: Array[T], idx: Int): Long
   def array_update[T](arr: Array[T], idx: Int, elt: Long): Unit
   ...
 } 
 object IntDispatcher extends Dispatcher { ... }
\end{lstlisting-nobreak}

Dispatcher objects are passed to the miniboxed class during instantiation and have final semantics. In the |reverse| benchmark, this would replace the type byte switches by method invocations, which could be inlined. Dispatchers make forced inlining and loop unswitching redundant. With the final |dispatcher| field set at construction time, the |reverse_M| inner loop body can have array access inlined and optimized: (``miniboxed,  dispatcher'' in tables \ref{mbox:tbl-mb-simple} and \ref{mbox:tbl-mb-classloading})

\begin{lstlisting-nobreak}
 // inlined getElement:
 val el1: Long = dispatcher.array_get(...)
 val el2: Long = dispatcher.array_get(...)
 // inlined setElement:
 dispatcher.array_update(...)
 dispatcher.array_update(...)
\end{lstlisting-nobreak}

\topic{Despite their clear advantages, in practice dispatchers can be used with at most two different value types.} This happens because the HotSpot Java Virtual Machine inlines the dispatcher code at the call site and installs guards that check the object's runtime type. The inline cache works for two receivers, but if we try to swap the dispatcher a third time, the callsite becomes megamorphic. In the megamorphic state, the |array_get| and |array_set| code is not inlined, hence the disappointing results for the ``Multi Context'' scenario.  

\topic{Interestingly, specialization performs equally well in both ``Single Context'' and ``Multi Context'' scenarios.} The explanation lies in the bytecode duplication: each specialized class contains a different body for the reverse method, and the profiles for each method do not interact. Accordingly, the results for integers are not influenced by the other value types used. This insight motivated the load-time cloning and specialization, which is described in the next section.