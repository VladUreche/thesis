\section{Benchmarks}
\label{sec:benchmarks}
\label{sec:benchmarks:ad-hoc}

This section evaluates the experimental benefits of ADR transformations in targeted micro-benchmarks and in
the setting of a library and its clients.


\begin{table*}[t!]
  \centering
  \begin{tabularx}{\textwidth}{|g| *{6}{|Y}|} \hline
    \rowcolor{Gray}                                &               &                  & \multicolumn{2}{c|}{In-benchmark}    & \multicolumn{2}{c|}{Inter-benchmark} \\\cline{4-7}
    \rowcolor{Gray}\textbf{Benchmark}              & \textbf{Time} & \textbf{Speedup} & \textbf{Garbage}  & \textbf{GC time}  & \textbf{Garbage}  & \textbf{GC time} \\
    \rowcolor{Gray}                                &  (ms)         &                  & (MB)              & (ms)              & (MB)              & (ms)     \\ \hline
    10K GCD runs, original & 28.1 &    none &        0 &        0 &     13.5 &       13 \\
    10K GCD runs, class    & 12.5 &    2.2x &        0 &        0 &      2.5 &       10 \\
    10K GCD runs, boxed    & 15.0 &    1.9x &        0 &        0 &      8.7 &       11 \\
    10K GCD runs, unboxed  &  2.2 &   12.7x &        0 &        0 &      0.5 &        9 \\ \hline
  \end{tabularx}
  \vspace{-1.9mm}
  \caption{Greatest Common Divisor benchmark results.}
  \label{table:gcd}
\end{table*}

We ran the benchmarks on an Intel |i7-4702HQ| quad-core processor machine with the frequency fixed at |2.2GHz|, and |2GB| of RAM, running the Oracle Java SE |1.7.0_80-b15| distribution on Ubuntu |14.04 LTS|. To avoid the noise caused by the just-in-time (JIT) compiler and garbage collection (GC) cycles, we measured the running times using the ScalaMeter benchmarking platform \cite{scalameter}, which warms up the Java Virtual Machine according to statistically rigorous performance evaluation guidelines \cite{rigorous-java-benchmarking}.

\subsection{ADRT Micro-Benchmarks}

Our benchmarking platform, ScalaMeter, executes micro-benchmarks using the following recipe:
\begin{compactitem}
  \item First, fork a new JVM;
  \item Execute the benchmark several times to warm up the JVM, only measuring the noise;
  \item When the noise drops below a threshold, execute the benchmark and gather measurements;
\end{compactitem}

\vspace{0.5em}
\noindent
For each benchmark run, we monitor:
\begin{compactitem}
  \item The benchmark running time;
  \item GC cycles occurring during the run (in-benchmark);
  \item GC cycles occurring after the run (inter-benchmark);
\end{compactitem}

\vspace{0.5em}
\noindent
At the end of a cycle, we manually trigger a full GC cycle so the current run does not affect the next. The memory collected after the run (inter-benchmark) corresponds to the input and output data and any garbage produced by running the benchmarked code that was not automatically collected during its execution (in-benchmark).

This allows us to record the following parameters for each benchmark:

\begin{compactitem}
  \item Benchmark running time (ms)
  \item In-benchmark garbage collected (MB)
  \item In-benchmark GC pause time (ms)
  \item Inter-benchmark garbage collected (MB)
  \item Inter-benchmark GC pause time (ms)
\end{compactitem}

\vspace{0.5em}
\noindent
Since the ADR transformation is directly related to memory layout and, thus, to memory consumption, we paid special attention to GC cycles. Please notice that the benchmark running time includes the in-benchmark GC pause but not the inter-benchmark GC pause. This allows us to separately measure the speedups gained by avoiding GC cycles and from other factors, such as:

\begin{compactitem}
  \item Avoiding pointer dereferencing;
  \item Improving cache locality;
  \item Simplifying operations;
  \item Specializing operations;
  \item Lazyfying operations.
\end{compactitem}

\vspace{0.5em}
\noindent
For each benchmark, we broke down the transformation in several steps, which allowed us to quantify the exact contribution obtained by each transformation step. Unfortunately, due to space constraints, we cannot include the complete analysis in the paper. Interested readers can review it in the accompanying artifact or on the project website \cite{ildl-plugin-wiki}.

\vspace{0.5em}
\noindent
We chose representative micro-benchmarks in order to cover a wide range of transformations using the |adrt| scope:

\begin{compactitem}
\item the greatest common divisor algorithm, presented in \S\ref{sec:problem};
\item least squares benchmark + deforestation \cite{wadler-deforestation};
\item averaging sensor readings + array of struct;
\item computing the first 10000 Hamming numbers.
\end{compactitem}

\vspace{0.5em}
\noindent
All benchmarks are fully automated and use the |adrt| markers and transformation description objects. We will proceed to explain the transformation in each benchmark, but, due to space constraints, the full descriptions are only available on the website.

\subsubsection{The Gaussian Greatest Common Divisor}
is the running example described in \S\ref{sec:problem} and used throughout the paper. It is a numeric, CPU-bound benchmark, where the main slowdown is caused by heap allocations and GC cycles. We broke down the transformation into four steps, with the result shown in Table \ref{table:gcd}. None of the transformations triggered GC pauses during the measured runs, but they did produce different amounts of garbage objects:

\vspace{0.5em}
\noindent
\textbf{The ``original'' benchmark} does not apply any transformation, thus modeling Gaussian integers using Scala's |Tuple2| class. Due to limitations in the specialization \cite{iuli-thesis, specialization-iuli} translation in Scala, the memory footprint of |Tuple2| classes is larger than it should be.

\vspace{0.5em}
\noindent
\textbf{The ``class'' transformation} applies an |adrt| transformation which encodes Gaussian integers as our own |Complex| class, essentially retrofitting specialization. This obtains a 2x speed improvement and reduces the garbage by 5x:

\begin{lstlisting-nobreak}
case class Complex(_1: Int, _2: Int)
\end{lstlisting-nobreak}

\vspace{0.5em}
\noindent
\textbf{The ``boxed'' transformation} encodes Gaussian integers as long integers, but keeps them heap-allocated. This is slower than having our own class since it requires encoding values into the long integer representation. To achieve boxing, we use |java.lang.Long| objects, which the Scala backend does not unbox. The additional value encoding produces a small slowdown and for unknown reasons increases the garbage produced.

\vspace{0.5em}
\noindent
\textbf{The ``unboxed'' transformation} is the one shown throughout the paper. It encodes Gaussian integers as |scala.Long| values, which are automatically unboxed by the Scala compiler backend. This brings a significant speedup to the benchmark, allowing execution to occur without any heap allocation, as explained in \S\ref{sec:ildl:method}. Compared to using pairs of integers, the speedup is almost 13x and the garbage is reduced by
27x.

\vspace{0.5em}
\noindent
The transformation description objects for the three transformations above range between 30 and 40 lines of code and include more operations than necessary for the benchmark, such as addition, multiplication, multiplication with integers, subtraction, etc.

\begin{table*}[t!]
  \centering
  \begin{tabularx}{\textwidth}{|g| *{6}{|Y}|} \hline
    \rowcolor{Gray}                                &               &                  & \multicolumn{2}{c|}{In-benchmark}    & \multicolumn{2}{c|}{Inter-benchmark} \\\cline{4-7}
    \rowcolor{Gray}\textbf{Benchmark}              & \textbf{Time} & \textbf{Speedup} & \textbf{Garbage}  & \textbf{GC time}  & \textbf{Garbage}  & \textbf{GC time} \\
    \rowcolor{Gray}                                &  (ms)              &             & (MB)              & (ms)              & (MB)              & (ms)     \\ \hline
    LSM, original          & 8264 &    none &     1166 &     7547 &      809 &     5317 \\
    LSM, scala-blitz       & 3464 &    2.4x &      468 &     2936 &     1165 &     5236 \\
    LSM, adrt generic      &  429 &   19.3x &      701 &        3 &      933 &     5210 \\
    LSM, adrt miniboxed    &  280 &   29.5x &        0 &        0 &      701 &     5193 \\
    LSM, manual deforestation  &  195 &   42.4x &        0 &        0 &      702 &     5269 \\
    LSM, manual fusion     &   79 &  105.0x &        0 &        0 &      702 &     5282 \\ \hline
  \end{tabularx}
  \vspace{-1.9mm}
  \caption{Least Squares Method benchmark results.}
  \label{table:lslr}
\end{table*}

\subsubsection{The Least Squares Method} takes a list of points in two dimensions and computes the slope and offset of a straight line that best approximates the input data. The benchmark performs multiple traversals over the input data and thus can benefit from deforestation \cite{wadler-deforestation}, which avoids the creation of intermediate collections after each |map| operation:

\begin{lstlisting-nobreak}
adrt(ListAsLazyList){
  def leastSquares(data: List[(Double, Double)]) = {
    val size = data.length
    val sumx = data.map(_._1).sum
    val sumy = data.map(_._2).sum
    val sumxy = data.map(p => p._1 * p._2).sum
    val sumxx = data.map(p => p._1 * p._1).sum
    ...
  }
}
\end{lstlisting-nobreak}

\noindent
The |adrt| scope performs a generic transformation from |List[T]| to |LazyList[T]|:

\begin{lstlisting-nobreak}
object ListAsLazyList extends TransformationDescription {
  def toRepr[T](list: List[T]): LazyList[T] = ...
  def toHigh[T](list: LazyList[T]): List[T] = ...
  // bypass methods
}
\end{lstlisting-nobreak}

The |LazyList| collection achieves deforestation by recording the mapped functions and executing them lazily, either when |force| is invoked on the collection or when a |fold| operation is executed. Since the |sum| operation is implemented as a |foldLeft|, the |LazyList| applies the function and sums the result without creating an intermediate collection.

To put the transformation into context, we explored several scenarios:

\vspace{0.5em}
\noindent
\textbf{The ``original'' case} executes the least squares method on 5 million points without any transformation. Table \ref{table:lslr} shows that, on average, as much as 1.1 GB of heap memory is reclaimed during the benchmark run, significantly slowing down the execution. If it was not for the in-benchmark GC pause, the execution would take around 700ms, in line with the other transformations.

\noindent
What we can also notice is that, across all benchmarks, the input data occupies around 700MB of heap space and is only collected at the end of the benchmark. A back-of-the-envelope calculation can confirm this: each linked list node takes 32 bytes (2-word header + 8-byte pointer to value + 8-byte pointer to the next cell) and contains a tuple of 48 bytes (2-word header + two 8-byte pointers and two 8-byte doubles, due to limitations in specialization), which itself contains 16 bytes per boxed double. Considering 5 million such nodes, we have: $(32 + 48 + 2 \times 16) * 5 \times 10^6 = 560 \times 10^6$, approximately 560MB of data.

\vspace{0.5em}
\noindent
\textbf{The ``blitz'' transformation} uses the dedicated collection optimization tool |scalablitz| \cite{scalablitz, scalablitz-paper} to improve performance. Under the hood, scalablitz uses compile-time macros to rewrite the code and improve its performance. Indeed, the tool manages to both cut down on garbage generation and improve the running performance of the code.

\vspace{0.5em}
\noindent
\textbf{The ``adrt'' transformation} performs deforestation by automatically introducing |LazyList|s. This avoids the creation of intermediate lists and thus significantly reduces the garbage produced. We tried using two versions of |LazyList|: one using erased generics (adrt generic) and one using miniboxing \cite{miniboxing} specialization (adrt miniboxed).

The erased generic |LazyList| executed the code on par with the scalablitz optimizer but produced less garbage and the GC pause was much shorter (probably requiring a simple young-generation collection, not a full mark and sweep).

The miniboxed |LazyList|, on the other hand, both executed faster and did not produce any in-benchmark garbage. If we count in-benchmark GC pauses, the speedup produced by combining ``adrt'' scopes for deforestation and miniboxing for specialization is 29.5x compared to the original code. If we only count execution time, subtracting in-benchmark GC pauses, the speedup is 2.56x.

\begin{table*}[t!]
  \centering
  \begin{tabularx}{\textwidth}{|g| *{6}{|Y}|} \hline
    \rowcolor{Gray}                                &               &                  & \multicolumn{2}{c|}{In-benchmark}    & \multicolumn{2}{c|}{Inter-benchmark} \\\cline{4-7}
    \rowcolor{Gray}\textbf{Benchmark}              & \textbf{Time} & \textbf{Speedup} & \textbf{Garbage}  & \textbf{GC time}  & \textbf{Garbage}  & \textbf{GC time} \\
    \rowcolor{Gray}                                &  (ms)              &             & (MB)              & (ms)              & (MB)              & (ms)     \\ \hline
    array of struct, random & 55.5 &    none &        0 &        0 &      451 &       15 \\
    struct of array, random & 30.4 &    1.8x &        0 &        0 &      435 &       13 \\
    array of struct, uniform& 32.5 &    none &        0 &        0 &      454 &       16 \\
    struct of array, uniform&  5.7 &    5.7x &        0 &        0 &      433 &       19 \\ \hline
    10001-th number, original  & 6.56 &    none &        0 &        0 &       31 &       11 \\
    10001-th number, step 1 & 2.70 &    2.4x &        0 &        0 &       31 &       11 \\
    10001-th number, step 2 & 2.16 &    3.0x &        0 &        0 &       31 &       12 \\
    10001-th number, step 3 & 1.64 &    4.0x &        0 &        0 &       31 &       10 \\ \hline
  \end{tabularx}
  \vspace{-1.9mm}
  \caption{Sensor Readings and Hamming Numbers benchmark results.}
  \label{table:sparkle}
%   \vspace{-1em }
\end{table*}

\vspace{0.5em}
\noindent
\textbf{Manual transformations} complete the picture: in the ``deforestation'' transformation we write C-like while loops by hand to traverse the input list. We use four separate loops, to simulate the best case scenario for an automated transformation. The result is a 1.43x speedup compared to ``adrt miniboxed''.

The ``fusion'' manual transformation unites the four separate input list traversals into a single traversal. While this transformation cannot be applied unless we assume a closed world, it is still interesting to compare our transformation to a best-case scenario. The manual fusion improves the performance by 3.54x compared to ``adrt miniboxed''. However, what we can notice is that both ``adrt miniboxed'' and the manual transformations produce the exact same amount of garbage: 700MB.

In terms of programmer effort, the |LazyList| definition takes about 60 LOC and the transformation description object about 30 LOC. The difference between ``adrt erased'' and ``adrt miniboxed'' is the presence of |@miniboxed| annotations in the |LazyList| classes and in the description object.

\subsubsection{The Sensor Readings} benchmark is inspired by the Sparkle visualization tool \cite{sparkle}, which is able to quickly display, zoom, transform and filter sensor readings. To obtain nearly real-time results, Sparkle combines several optimizations such as streaming and array-of-struct to struct-of-array conversions, all currently implemented by hand. In our benchmark, we implemented a mock-up of the Sparkle processing core and automated the array-of-struct to struct-of-array transform:

\begin{lstlisting-nobreak}
type SensorReadings = Array[(Long, Long, Double)]
class StructOfArray(arrayOfTimestamps: Array[Long],
                           arrayOfEvents:     Array[Long],
                           arrayOfReadings:   Array[Double])

object AoSToSoA extends TransformationDescription {
  def toRepr(aos: SensorReadings): StructOfArray = ...
  def toHigh(soa: StructOfArray): SensorReadings = ...
  ...
}
\end{lstlisting-nobreak}

In the benchmark, we have an array of 5 million events, each with its own timestamp, type and reading. We are seeking to average the readings of a single type of event occurring in the event array. Since our transformation influences cache locality, we had two different speedups depending on the event distribution:

\begin{compactitem}
 \item Randomly occurring events are triggered with a probability of 1/3 in the sensor reading array;
 \item Uniformly occurring events appear every 3rd element, thus offering more room for CPU speculation.
\end{compactitem}

Using the |adrt| scope to transform the array of tuples into a tuple of arrays allows better cache locality and fewer pointer dereferences. With random events, the ``adrt'' transformation produces a speedup of 1.8x. With uniformly distributed events, both the original and the transformed code run faster, yet resulting in a speedup of 5.7x.

In all four cases, the amount of memory allocated is approximately the same and no objects are allocated aside from the input data. Thus, the operation speedups are obtained through improving cache locality.

The transformation description object is 50 LOC and requires 20 additional LOC to define implicit conversions.

\subsubsection{The Hamming Numbers Benchmark} computes numbers that only have 2, 3 and 5 as their prime factors, in order. Unlike the other benchmarks, this is an example we randomly picked from Rosetta Code \cite{rosetta-code} and attempted to speed up:

\begin{lstlisting-nobreak}
adrt(BigIntToLong) {
  adrt(QueueOfBigIntAsFunnyQueue) {
    class Hamming extends Iterator[BigInt] {
      import scala.collection.mutable.Queue
      val q2 = new Queue[BigInt]
      val q3 = new Queue[BigInt]
      val q5 = new Queue[BigInt]
      def enqueue(n: BigInt) = {
        q2 enqueue n * 2
        q3 enqueue n * 3
        q5 enqueue n * 5
      }
      def next = {
        val n = q2.head min q3.head min q5.head
        if (q2.head == n) q2.dequeue
        if (q3.head == n) q3.dequeue
        if (q5.head == n) q5.dequeue
        enqueue(n); n
      }
      def hasNext = true
      q2 enqueue 1
      q3 enqueue 1
      q5 enqueue 1
    }
  }
}
\end{lstlisting-nobreak}

An observation is that, for the first 10000 Hamming numbers, there is no need to use |BigInt|, since the numbers fit into a |Long| integer. Therefore, we used two nested |adrt| scopes to replace |BigInt| by |Long| and |Queue[BigIng]| by a fixed-size circular buffer built on an array. The result was an 4x speedup. The main point in the transformation is its optimistic nature, which makes the assumption that, for the Hamming numbers we plan to extract, the long integer and a fixed-size circular buffer are good enough. This is similar to what a dynamic language virtual machine would do: it would make assumptions based on the code and would automatically de-specialize the code if the assumption is invalidated. In our case, when the assumption is invalidated, the code will throw an exception.

As with other benchmarks, we broke down the transformation is several steps:

\vspace{0.5em}
\noindent
\textbf{The ``original'' code} is the unmodified version from the Rosetta Code website, which we kept as a witness.

\vspace{0.5em}
\noindent
\textbf{The ``step1'' code} uses |adrt| scopes to replace the |Queue| object with a custom, fixed-size array-based circular buffer. This collection specialization brings a 2.4x speedup without any memory layout transformation.

\vspace{0.5em}
\noindent
\textbf{The ``step2'' code} uses |adrt| scopes to replace the |BigInt| object in both class |Hamming| and the circular buffer by boxed |java.lang.Long| objects. This additional range restriction brings an extra 1.25x speedup.

\vspace{0.5em}
\noindent
\textbf{The ``step3'' code} replaces the |BigInt| objects by unboxed |scala.Long| values. This unboxing operation produces an additional 1.31x speedup, as fewer objects are created during the benchmark execution.

The conclusion is that, although the ADR transformation can be viewed as a memory layout optimization, it can additionally trigger more optimizations that bring orthogonal speedups, such as
specializing operations and collections.

For this example, the two transformation objects are 100 LOC and the circular buffer is another 20 LOC.

\subsection{ADRT in Realistic Libraries}
\label{sec:benchmarks:funcs}

\begin{table}[t]
  \begin{tabularx}{0.48\textwidth}{|g *{3}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark} & \textbf{Generic} & \textbf{Miniboxed}& \textbf{Miniboxed} \\
    \rowcolor{Gray}
                       &                  &                   & +functions \\ \hline
    Sum                &          98.2 ms &          158.6 ms &             18.0 ms \\
    SumOfSquares       &         131.6 ms &          193.1 ms &             12.0 ms \\
    SumOfSqEven        &          92.3 ms &          189.6 ms &             48.7 ms \\
    Cart               &         217.4 ms &          214.9 ms &             57.5 ms \\ \hline
  \end{tabularx}
  \vspace{-2mm}
  \caption{Scala Streams pipelines for 10M elements.}
  \label{table:streams}
  \vspace{-1em}
\end{table}

The |adrt| scoped transformation is a conceptual generalization of a mechanism
motivated by library transformation scenarios. In particular, the
resulting data representation transformation is used in conjunction
with the miniboxing transformation \cite{miniboxing-www, miniboxing},
in order to replace standard library \emph{functions} and \emph{tuples}
by custom, optimized versions adequate for miniboxed code \cite{miniboxing-pppj}.
The scope of this data representation transformation is miniboxing-transformed code.

The miniboxing transformation \cite{miniboxing} proposes an alternative to erasure, allowing generic methods and classes to work efficiently with unboxed primitive types. Unlike the current specialization transformation in the Scala compiler \cite{iuli-thesis}, which duplicates and adapts the generic code once for every primitive type, the miniboxing transformation only duplicates the code once and \emph{encodes all primitive types in long integers}. This allows miniboxing to scale much better than specialization \cite{miniboxing-linkedlist} in terms of bytecode size while providing comparable performance. Yet, one of the main drawbacks of using the miniboxing plugin is that all Scala library classes are either generic or specialized with the built-in Scala specialization scheme, which is not compatible with miniboxing. Therefore, interacting with functions and tuples from miniboxed code incurs significant overhead.

Consider, for example, functions. (Tuples raise similar issues.) Scala offers functions as first-class citizens. However, since functions are not first-class citizens in the Java Virtual Machine bytecode, the Scala compiler desugars them to anonymous classes extending a functional interface. The following example shows the desugaring of function |(x: Int) => x + 1|:

\begin{lstlisting-nobreak}
class $anon extends Function1[Int, Int] {
  def apply(x: Int): Int = x + 1
}
new $anon()
\end{lstlisting-nobreak}

This function desugaring does not expose a version of the |apply| method that encodes the primitive type as a long integer, as the miniboxing transformation expects. Therefore, when programmers write miniboxed code that uses functions, they have two choices: either accept the slowdown caused by converting the representation or define their own miniboxed |Function1| class, and perform the function desugaring by hand. Neither of these is a good solution.

Our data representation transformation converts the references to |Function1| in miniboxed code to the optimized |MiniboxedFunction1|, which allows calls to use the miniboxed representation, thus being more efficient. The problem is that the miniboxed code needs to interoperate with library-defined code, or with other libraries that were not transformed. Thus, the miniboxed code acts as a scope for the \emph{function and tuple representation transformation}, i.e., the ADR transformation of |Function| and |Tuple|. This transformation has a significant impact in library benchmarks.

\begin{table}[t]
  \begin{tabularx}{0.48\textwidth}{|g *{1}{|Y}|} \hline
    \rowcolor{Gray}
    \textbf{Benchmark}             &  \textbf{Running time} \\ \hline
    Manual C-like code             &         0.650 $\mu$s \\
    Miniboxing with functions      &         0.705 $\mu$s \\
    Miniboxing without functions   &         3.080 $\mu$s \\
    Generic                        &        13.409 $\mu$s \\ \hline
  \end{tabularx}
  \vspace{-2mm}
  \caption{Mapping a 1K vector.}
  \label{table:framian}
  \vspace{-1em}
\end{table}

\subsubsection{The Scala-Streams library} \cite{biboudis_clash_2014} imitates the design of the Java 8 stream library, to achieve high performance (relative to standard Scala libraries) for functional operations on data streams. The library is available as an open-source implementation \cite{biboudis-streams}. In its continuation-based design, each stream combinator provides a function that is stacked to form a transformation pipeline. As the consumer reads from the final stream, the transformation pipeline is executed, processing an element from the source into an output element. However, the pipeline architecture is complex, since combinators such as |filter| may drop elements, stalling the pipeline.

Table \ref{table:streams} shows the result of applying our data
representation transformation to the Scala-Streams published
benchmarks. (The benchmarks are described in detail in prior
literature \cite{biboudis_clash_2014,biboudis_et_al:ECOOP:short}.) As can be seen, the miniboxing
transformation is an enabler of our optimization but produces
\emph{worse} results by itself (due to extra conversions).

Compared to the original library, the application of miniboxing and
data representation optimization for functions achieves a very high
speedup---up to 11x for the SumOfSquares benchmark. In fact, the
speedup relative to the miniboxed code without the function
representation optimization is nearly 16x! \iv{Miniboxing -- it really whips the llama's ass!}


\subsubsection{The Framian Vector implementation} is an exploration into deeply specializing the immutable |Vector| bulk storage without using reified types \cite{tixxit-respecialization15,tixxit-respecialization6}. This is a benchmark created by a third party (a commercial entity using Scala). Table \ref{table:framian} shows a 4.4x speed improvement when the function representation is optimized and shows that the ADR-transformed function code performs within 10\% of the fully specialized and manually optimized code. \iv{Erm, no, that was Winamp with the llama, not miniboxing...}
\vspace{-1em}
