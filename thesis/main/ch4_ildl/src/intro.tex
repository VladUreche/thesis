\section{Introduction}
\label{sec:intro}

An object encapsulates code and data and exposes an interface. Modern language facilities, such as extension methods, type classes and implicit conversions allow programmers to evolve the object interface in an ad hoc way, by adding new methods and operators. For example, in Scala, we can use an implicit conversion to add the multiplication operator to pairs of integers, with the semantics of complex number multiplication:

\begin{lstlisting-nobreak}
scala> (0, 1) * (0, 1)
res0: (Int, Int) = (-1, 0)
\end{lstlisting-nobreak}

Unlike evolving the interface, there is no mechanism in modern languages for evolving an object's encapsulated data as the programmer sees fit. The encapsulated data format is assumed to be fixed, allowing the compiled code to contain hard references to data, encoded according to a convention known as the \emph{object layout}. For instance, methods encapsulated by the generic pair class, such as |swap| and |toString|, rely on the existence of two generic fields, erased to |Object|. This leads to inefficient storage in our running example, as the integers need to be boxed, producing as many as 3 heap objects for each ``complex number'': the two boxed integers and the pair container. What if, for a part of  our program, instead of the pair, we concatenated the two 32-bit integers into a 64-bit long integer, that would represent the ``complex number''? We could pass complex numbers by value, avoiding the memory allocation and thus the garbage collection cost. Additionally, what if we could also add functionality, such as arithmetic operations, directly on our ad hoc complex numbers, without any heap allocation overhead?

Object layout transformations are common in dynamic language virtual machines, such as V8, PyPy and Truffle. These virtual machines profile values at run-time and make optimistic assumptions about the shape of
objects. This allows them to improve the object layout in the heap, at the cost of recompiling all of the code that references the old object layout. If, later in the execution, the assumptions prove too optimistic, the virtual machine needs to revert to the more general (and less efficient) object layout, again recompiling all the code that contains hard references to the optimized layout. As expected, this comes with significant overheads. Thus, runtime decisions to change the low-level layout are expensive (due to recompilation) and have a global nature, affecting all code that assumes a certain layout.

Since transforming the object layout at run-time is expensive, a natural question to ask is whether we can leverage the statically-typed nature of a programming language to optimize the object layout during compilation? The answer is yes. Transformations such as ``class specialization'' and ``value class inlining'' transform the object layout in order to avoid the creation of heap objects. However, both
of these transformations take a global approach: when a class is marked as specialized or as a value class (and assuming it satisfies the semantic restrictions) it is transformed at its definition site. Later
on, this allows all references to the class, even in separately compiled sources, to be optimized. On the other hand, if a class is not marked at its definition site, retrofitting specialization or the value class status is impossible, as it would break many non-orthogonal language features, such as dynamic dispatch, inheritance and generics.

Therefore, although transformations in statically typed languages can optimize the object layout, they do not meet the ad hoc criterion: they cannot be retrofitted later, and they have a global, all-or-nothing
nature. For instance, in Scala, the generic pair class is specialized but not marked as a value class. As a result, the representation is not fully optimized, still requiring a heap object for each pair. Even
worse, specialization and value class inlining are mutually exclusive, making it impossible to optimally represent our ``complex numbers'' even if we had complete control over the Scala library. Furthermore, our encoded ``complex number'' data representation may be applicable for specific parts of the client code, but might not make sense globally.

In our ``complex numbers'' abstraction, we only use a fraction of the flexibility provided by the library tuples, and yet we have to give up all the code optimality. Even worse, for our limited domain, we are aware of a better representation, but the only solution is to transform the code by hand, essentially having to choose between an obfuscated or a slow version of the code. What is missing is a largely automated and safe transformation that allows us to use our domain-specific knowledge to mark a scope where the ``complex numbers'' can use the encoded representation, effectively specializing that part of our program.

In this paper we present such an automated transformation that allows programmers to safely change the data representation in limited, well-defined scopes that can include anything from expressions to method and class definitions. The transformation, which occurs during compilation, maintains strong correctness guarantees in terms of non-orthogonal language features, such as dynamic dispatch, inheritance and generics, while also maintaining consistence across separate compilation runs.

Like metaprogramming, which allows developers to transform their code in an ad-hoc ways, our technique allows redefining the data representation to be used inside delimited scopes. Because of its power, the technique also affords potential for misuse. In some cases, specifically for mutable and reference-based data structures, the transformations must be carefully designed to preserve language semantics
(\S\ref{sec:ildl:language-features}). Still, altering program semantics may be desirable---we exploit this property in the deforestation benchmark, shown in the evaluation section (\S\ref{sec:benchmarks:ad-hoc}).

The scoped nature of the transformation tightly controls which parts of the code use the new data representation and operations while the mechanism for defining transformations automatically eliminates
many of the common semantics-altering pitfalls. Given a programmer-designed data representation transformation, inside the delimited scopes the compiler is responsible for: (1) automatically deciding when to apply the transformation and when to revert it, in order to ensure correct interchange between representations, (2) enriching the transformation with automatically generated bridge code that ensures correctness relative to overriding and dynamic dispatch and (3) persisting the necessary metadata to allow transformed program scopes in different source files and compilation runs to communicate using the optimized representation---a property we refer to as \emph{composability} in the following sections. Thus, our approach adheres to the design principle of separating the reusable, general and provably correct transformation \emph{mechanism} from the programmer-defined \emph{policy}, which may contain incorrect decisions \cite{lampson-mechanism-policy}. In this context, our main contributions are:
\vspace{0.25em}
\begin{compactitem}
  \item Introducing the data representation metaprogramming problem, which, to the best of our knowledge, has not been addressed at all in the literature (\S\ref{sec:problem});
  \item Presenting the extensions that allow global data representation transformations (\S\ref{sec:drt}) to be used as scoped programmer-driven transformations (\S\ref{sec:ildl});
  \item Implementing the approach presented as a Scala compiler plugin \cite{ildl-plugin} that allows programmers to express custom transformations (\S\ref{sec:impl}) and benchmarking the plugin on a broad spectrum of transformations, ranging from improving the data layout and encoding, to retrofitting specialization and value class status, and to collection deforestation \cite{wadler-deforestation}. These transformations produced  speedups between 1.8 and 24.5x on user programs (\S\ref{sec:benchmarks}).
\end{compactitem}
